# 决策树算法之机器学习记录学习历程
​        最近在学习机器学习实战这本书，先将书上的代码整体过了一遍，在这个过程中CSDN上的一篇文章给我帮助很大，在此表示感谢。此博文的链接 https://blog.csdn.net/cxjoker/article/details/79501887 。整个学习过程差不多花了三个星期，本来以为很快就能完成，无奈刚接触很多知识点都要百度，很多不理解的代码都是一步步打印运行，特别是递归这一块。在终于差不多理解完后，在对UCI上的数据集wine和car还有鸢尾花进行测试时，遇到了一大推问题，又开始学习如何处理数据集，最后总算是将准确率给提升上去了。现在就记录一下大致的历程，以供日后查看学习。
​       在这个过程中提出了以下三个问题：第一数据集中有重复的数据集是否会影响特征的选择？第二，如何处理数据集可提高测试的准确性？第三，数据集中的数据如果过多是否会产生过拟合？如果是，是先删除部分数据再构建决策树还是先构建决策树再进行剪枝？对第一个问题，通过不断修改数据集进行验证，发现数据集中的重复项并不会影响特征的选择，只会影响整体的信息增益。对第二个问题，手动随机选取car数据集中的一部分数据进行训练，发现对同一个测试集正确率从50%到70%跳动；使用自动交叉验证方法，正确率也没有超过70%；对wine数据进行自动交叉验证时，正确率只有25.9%，进行离散化处理后，准确率提升到92.6%；对鸢尾花数据据进行自动交叉验证时正确率是73%，进行离散化处理后正确率到了73%。可见，将连续数据进行离散化后会提高决策树的正确性。对于理论性的知识可以参考上段中的博文。

​			对于离散型的数据整个构造决策树的代码如下

这一段代码是计算信息熵，为后面计算信息增益做准备

```python
def calcShannonEnt(dataSet):    
    numEntries=len(dataSet)   #获取数据集的长度    
    #print(numEntries)    
    labelCounts={}            #定义一个空字典    
    for featVec in dataSet:        
        currentLabel=featVec[-1]  #取出数据集中每一个列表的最后一个元素：‘no’or‘yes’        
        if currentLabel not in labelCounts.keys():            
            labelCounts[currentLabel]=0   #对字典进行操作，如果数据标签没有出现在labelcounts中就加进去，如果已出现了就值加1            
            #print(labelCounts)        
            labelCounts[currentLabel]+=1        #这段的功能是统计每个‘no’和‘yes’在数据集中出现的次数        #print(labelCounts)    
            shannonEnt=0.0    
            for key in labelCounts:                 #计算香农熵        		
                prob=float(labelCounts[key])/numEntries        
                shannonEnt-=prob*log(prob,2)        
                #print(prob)    
       return shannonEnt
```

接下来就是利用信息熵来计算信息增益，但是由于数据集中一般不只一个属性值，因此在调用calcShannonEnt函数时，必须先将数据集进行拆分。以下就是拆分数据集的函数

```python
def splitDataSet(dataSet,axis,value):   #axis是特征在列表中的索引，value是要匹配的特征值 
    retDataSet=[]                       #python传递的是列表的引用，因此要创建一个新的列表    
    for featVec in dataSet:        
        if featVec[axis]==value:                    #将数据集中与给定特征相匹配的数据抽取出来放在retdataset中            
            reduceFeatVec=featVec[:axis]            
            reduceFeatVec.extend(featVec[axis+1:])      #这两行代码巧妙的将符合特征值的特征向量结合到一起            
            retDataSet.append(reduceFeatVec)            #（以鱼的数据举例：[1,1,'yes']是数据集中的一个数据，   
     return retDataSet                                   #前两个元素是特征，如果第一个元素匹配上了，则resetdataset中会出现[1,'yes'],除去了第一个元素
```

接下来就是计算每个特征的信息增益，选出最优的划分方法

```python
def chooseBestFeatureToSplit(dataSet):                      #计算信息增益，根据信息增益最大的特征值来划分数据集    
    numFeatures=len(dataSet[0])-1                           #计算特征数量，最后一项是类别特征所以要减1    
    baseEntropy=calcShannonEnt(dataSet)                     #计算数据集的信息熵    
    bestInfoGain=0.0    
    bestFeature=-1                                          #为避免特征的干扰，设为负数    
    for i in range(numFeatures):                            #对每一个特征进行提取（看每个特征包含哪些取值，此处只有0/1两种取值        
        featList=[example[i] for example in dataSet]        
        #print(featList)        
        uniqueVals=set(featList)                            #去掉字典中重复的key，此时只有key              
        #print(uniqueVals)        
        newEntropy=0.0        
        for value in uniqueVals:                            #计算信息增益            
            subDataSet=splitDataSet(dataSet,i,value)            
            #print(subDataSet)            
            prob=len(subDataSet)/float(len(dataSet))            
            newEntropy+=prob*calcShannonEnt(subDataSet)        
            infoGain=baseEntropy-newEntropy        
            #print('this is the infoGain:',infoGain)        
            if(infoGain>bestInfoGain):                          #找出信息增益最大的特征            
                bestInfoGain=infoGain            
                #print('this is the bestinfoGain:',bestInfoGain)            
                bestFeature=i    
                if bestFeature==-1:        
                    raise ValueError('invalid value:%s'%(bestFeature))    
          return bestFeature
```

​			



接下来就是构建树了，在这之前还得先写一个投票函数，就是在特征已用完但是仍有分歧时使用投票表决的方法将叶子节点记为数量最多的那一项。然后利用递归的方法一步步扩大树。

```python
def majorityCnt(classList):                     #投票表决，选出数据集中类别最多的标签
    classCount={}
    for vote in classList:
        if vote not in classCount.key():
            classCount[vote] = 0
        classCount[vote]+=1
    sortedClassCount=sorted(classCount.items(),key=operator.itemgetter(1),reverse=Ture)
    return sortedClassCount[0][0]

def createTree(dataSet,labels):                             #构建递归树，为画递归树做准备
    classList=[example[-1] for example in dataSet]
    if classList.count(classList[0])==len(classList):
        #print(classList[0])
        return classList[0]                             #如果运行到此，则返回一个值不必运行下面的程序
    if len(dataSet[0])==1:
        return majorityCnt(classList)           #这一句的用途是？如果还没划分完，就将其划分为标签数最多的那一类
    bestFeat=chooseBestFeatureToSplit(dataSet)

    bestFeatLabel=labels[bestFeat]
    myTree={bestFeatLabel:{}}                   #下面的mytree如何跟这里的mytree结合起来？最后一次递归会成为上一次递归的值
    #print('this is mytree:',myTree)
    del(labels[bestFeat])
    #print('this is labels:',labels)
    #print('if the dataSet changes:',dataSet)
    featValues=[example[bestFeat] for example in dataSet]
    uniqueVals=set(featValues)
    for value in uniqueVals:
        subLabels=labels[:]
       # print('this is the bestFeat:',bestFeat)
        #print(value)
        #print('this is sublabels:',subLabels)
        #print(splitDataSet(dataSet, bestFeat, value))
        #print('if this is the samw:',dataSet)
        myTree[bestFeatLabel][value]=createTree(splitDataSet(dataSet,bestFeat,value),subLabels) #递归的myTree为啥没有传递到下次递归中？因为函数还没有返回mytree
        #print('this is a new mytree:',myTree)
    return myTree
```

对于画决策树的代码此处就不展现，详情请看源代码

接下来就是对数据进行测试了，用机器学习实战上面的眼镜数据集得到的结果非常好，但是用UCI上的car数据集，很多问题就暴露出来了。首先是car数据集中有1728行数据，此数据太大导致在pycharm上运行时无法运行，暂时还没解决。最先不知道可以利用代码将数据集进行分割成训练集和测试集，走了很多弯路。当时是手动选择一些数据作为训练集和测试集（两者无交集）。运行效果不好，就开始思考是哪里出了问题，猜想应该是训练集的数据不够全面导致决策树无法反映car数据的全部属性导致测试效果不好。然后就选择了一些比较全面的数据，提升了正确率，但是依然比较低。在这个过程中，思考如果以后都是这们处理数据那不是非常麻烦。网上找资料 发现是有代码可以随机选择数据作为训练集和测试集（也使用于深度学习中）。遂开始使用代码随机分割数据，在此运行，正确率并没有提升很多，依然在70%左右。继续思考是否是出现了过拟合导致正确率不高。这个时候有用了wine数据集和iris数据集，wine数据集只能达到20%多的正确率，iris数据集可以达到70多的正确率。继续找资料，发现wine和iris数据是连续型的数据，而之前的代码都是针对离散型的数据。刚好《机器学习》书上有讲如何讲连续数据进行离散化处理。发现处理后，正确率明显提升很多，树也精简了很多。后续在看进行剪枝后是否会有帮助。

如下代码是测试iris数据集的代码，用了离散和未离散进行对比，因此代码比较长（对数据离散化处理构造决策树的代码祥见源代码

```python
import treePlotter
import trees
import treescontinuous
fr=open('wine_train.txt')
#lenses=[inst.strip().split('\t') for inst in fr.readline()]     #此处不能使用readline，否则返回的数据只有一行且分割为单个字母
wine=[inst.strip().split(',') for inst in fr.readlines()]
#print(car)
wineLabels=['Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']
wineLabels1=['Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']
wineLabelsProperty=[1,1,1,1,1,1,1,1,1,1,1,1,1]
wineLabelsProperty1=[1,1,1,1,1,1,1,1,1,1,1,1,1]
#wineTree=trees.createTree(wine,wineLabels)                    #必须要带上tree才不会报错
wineTree=treescontinuous.createTree(wine,wineLabels,wineLabelsProperty)
#print(carTree)
treePlotter.createPlot(wineTree)

def classify(inputTree,featLabels,testVec):
    classLabel=''                          #此处如果不加这段代码时就会报以下错误
                                            # UnboundLocalError: local variable 'classLabel' referenced before assignment
    #global classLabel                      #如果加上这个代码会报以下错误SyntaxError: name 'classLabel' is assigned to before global declaration
                                            #另外如果将此代码运行，classLabel=''代码屏蔽，则会出现不正确的运行结果或者出现classLael没有定义
    firstStr=list(inputTree.keys())[0]
    #print('this is the firstStr:',inputTree.keys())
    secondDict=inputTree[firstStr]
    #print('this is the seconddict:',secondDict)
    featIndex=featLabels.index(firstStr)
    #print('this is the featindex:',featIndex)
    for key in secondDict.keys():
        if testVec[featIndex]==key:
            if type(secondDict[key]).__name__=='dict':
                classLabel=classify(secondDict[key],featLabels,testVec)
            else:
                classLabel=secondDict[key]
                #print('this is the classLabel:',classLabel)
    return classLabel

def classify_c(inputTree,featLabels,featLabelProperties,testVec):
    firstStr=list(inputTree.keys())[0]
    firstLabel=firstStr
    lessIndex=str(firstStr).find('<')
    if lessIndex>-1:
        firstLabel=str(firstStr)[:lessIndex]
    secondDict=inputTree[firstStr]
    featIndex=featLabels.index(firstLabel)
    classLabel=None
    for key in secondDict.keys():
        if featLabelProperties[featIndex]==0:
            if testVec[featIndex]==key:
                if type(secondDict[key]).__name__=='dict':
                    classLabel=classify_c(secondDict[key],featLabels,featLabelProperties,testVec)
                else:
                    classLabel=secondDict[key]
        else:
            partValue=float(str(firstStr)[lessIndex+1:])
            if float(testVec[featIndex])<partValue:
                #if float(testVec[featIndex]) < partValue:          #此处要加疯老头，不然就成了sting类型的
                if type(secondDict['是']).__name__=='dict':
                    classLabel=classify_c(secondDict['是'],featLabels,featLabelProperties,testVec)
                else:
                    classLabel=secondDict['是']
            else:
                if type(secondDict['否']).__name__=='dict':
                    classLabel=classify_c(secondDict['否'],featLabels,featLabelProperties,testVec)
                else:
                    classLabel=secondDict['否']
    return classLabel


def testacuccy(inputTree,featLabel,testdataset):
    testdatasetnum=len(testdataset)
    print(testdatasetnum)
    rightnum=0
    for i in range(testdatasetnum):
        testdatasetvec=testdataset[i][:-1]
        testresult=classify(inputTree,featLabel,testdatasetvec)
        if testresult==testdataset[i][-1]:
            rightnum+=1
    print(rightnum)
    accuracy=float(rightnum/testdatasetnum)
    return accuracy

def testacuccy_c(inputTree,featLabel,featLabelProperties,testdataset):
    testdatasetnum=len(testdataset)
    print(testdatasetnum)
    rightnum=0
    for i in range(testdatasetnum):
        testdatasetvec=testdataset[i][:-1]
        testresult=classify_c(inputTree,featLabel,featLabelProperties,testdatasetvec)
        if testresult==testdataset[i][-1]:
            rightnum+=1
    print(rightnum)
    accuracy=float(rightnum/testdatasetnum)
    return accuracy

frtest=open('wine_test.txt')                                         #数据一定要保证标点符号符合规范，且最后不能有空行，否则会出现空列表导致运行出错
#lenses=[inst.strip().split('\t') for inst in fr.readline()]     #此处不能使用readline，否则返回的数据只有一行且分割为单个字母
winetest=[inst.strip().split(',') for inst in frtest.readlines()]   #正确率只有22%，初步分析结果应该是训练数据不全导致决策树不够全面，因此如何分析数据
                                                                    #选择恰当的数据进行训练尤为重要，将训练数据换成car traning2后准确率提升到了57%
                                                                    #将训练数据增加到93个后发现准确率较第二次反而降低了差不多一个百分点

#fineltestresult=testacuccy(wineTree,wineLabels1,winetest)
fineltestresult=testacuccy_c(wineTree,wineLabels1,wineLabelsProperty1,winetest)
print(fineltestresult)
```

​		另外一方面是如何将UCI上下载的数据变成程序需要的列表形式，这时候不能有空格，不能有空行，否则都会运行出错。

下面两图是对wine数据集进行离散化处理和未处理的对比图，可以看出离散化处理后树也简洁了很多。

![image-20191101194431494](C:\Users\S\AppData\Roaming\Typora\typora-user-images\image-20191101194431494.png)

![image-20191101194454000](C:\Users\S\AppData\Roaming\Typora\typora-user-images\image-20191101194454000.png)

仍需要解决的问题：1、大数据集如何运行；2、将构造树保存文件为何能缩短运行时间，下次如何使用；3、如何对树进行剪枝处理，如何判断过拟合？